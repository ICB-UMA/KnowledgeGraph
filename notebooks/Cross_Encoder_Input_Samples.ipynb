{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "import src.faissEncoder as faiss_enc\n",
    "from src.crossEncoder import CrossEncoderReranker\n",
    "from src.tripletsGeneration import  HardTripletsKG, SimilarityHardTriplets, TopHardTriplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    \"SapBERT-UMLS-2020AB-all-lang-from-XLMR/\": \"SapBERT_Multilingue_XLMR\",\n",
    "    \"SapBERT-UMLS-2020AB-all-lang-from-XLMR-large/\": \"SapBERT_Multilingue_XLMR-large\",\n",
    "    \"spanish_sapbert_models/sapbert_15_grandparents_1epoch/\": \"Spanish_SapBERT_grandparents\",\n",
    "    \"spanish_sapbert_models/sapbert_15_parents_1epoch/\": \"Spanish_SapBERT_parents\",\n",
    "    \"spanish_sapbert_models/sapbert_15_noparents_1epoch/\": \"Spanish_SapBERT_no_parents\",\n",
    "    \"sapbert_B_256_E_1/\" : \"Spanish_SapBERT_ICB\",\n",
    "    \"biencoder_symptemist_enriched_triplets_5_epoch_32_batch_5_parents_stag/\" : \"BiEncoder_SympTEMIST\",\n",
    "    \"biencoder_medprocner_enriched_triplets_1_epoch_32_batch_5_parents_stag/\" : \"BiEncoder_MedProcNER\",\n",
    "    \"biencoder_distemist_enriched_triplets_5_epoch_32_batch_5_parents_stag/\" : \"BiEncoder_DisTEMIST\",\n",
    "    \"roberta-base-biomedical-clinical-es/\": \"Roberta-base-biomedical-clinical-es\"\n",
    "}\n",
    "CORPUS = \"MedProcNER\"\n",
    "CORPUS_PATH = f\"../../EntityLinking/data/{CORPUS}/processed_data/\"\n",
    "MODEL_PATH = \"cambridgeltl/SapBERT-UMLS-2020AB-all-lang-from-XLMR\"\n",
    "HARD_TRIPLETS_TYPE = \"kg\"\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 1\n",
    "CANDIDATES = 200\n",
    "NUM_NEGATIVES = 200\n",
    "DEPTH = 1\n",
    "F_TYPE = \"FlatIP\"\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "EVAL_STEPS = 250000\n",
    "TEST_SIZE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL_PATH.split(\"/\")[-1]\n",
    "mapped_name = model_mapping.get(model, model)\n",
    "output_path = os.path.join(\"../models/\", f\"cef_{CORPUS.lower()}_{mapped_name}_{HARD_TRIPLETS_TYPE}_cand_{NUM_NEGATIVES}_epoch_{EPOCHS}_bs_{BATCH_SIZE}\" if DEPTH == 0 else f\"cef_{CORPUS.lower()}_{mapped_name}_{HARD_TRIPLETS_TYPE}_{DEPTH}_cand_{NUM_NEGATIVES}_epoch_{EPOCHS}_bs_{BATCH_SIZE}\")\n",
    "df_test = pd.read_csv(os.path.join(CORPUS_PATH, \"df_link_test.tsv\"), sep=\"\\t\", header=0, dtype={\"code\": str})\n",
    "df_unseen_codes = pd.read_csv(os.path.join(CORPUS_PATH, \"df_unseen_codes.tsv\"), header=0, sep='\\t', dtype={\"code\": str})\n",
    "df_train = pd.read_csv(os.path.join(CORPUS_PATH, \"df_link_gaz_train.tsv\"), sep=\"\\t\", header=0, dtype={\"code\": str})\n",
    "df_gaz = pd.read_csv(os.path.join(CORPUS_PATH, \"gazetteer_term_code.tsv\"), sep=\"\\t\", header=0, dtype={\"code\": str}, low_memory=False)\n",
    "df_train_link = pd.read_csv(os.path.join(CORPUS_PATH, \"df_link_train.tsv\"), sep=\"\\t\", header=0, dtype={\"code\": str}, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a673a21f684f0486f5a9e578aa1632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/7321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5378c8569ff347439bc0490ef98081d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'graph_G.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     df_hard_triplets \u001b[38;5;241m=\u001b[39m SimilarityHardTriplets(df_train_link)\u001b[38;5;241m.\u001b[39mgenerate_triplets(similarity_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.35\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m HARD_TRIPLETS_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph_G.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m         G \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscui_to_cui_dict.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n",
      "File \u001b[0;32m~/.conda/envs/tf-torch-nel/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'graph_G.pkl'"
     ]
    }
   ],
   "source": [
    "faiss_encoder = faiss_enc.FaissEncoder(MODEL_PATH, F_TYPE, MAX_LENGTH, df_gaz)\n",
    "faiss_encoder.fitFaiss()\n",
    "\n",
    "candidates, codes, similarities = faiss_encoder.getCandidates(df_train_link[\"term\"].tolist(), CANDIDATES , MAX_LENGTH)\n",
    "df_train_link[\"candidates\"], df_train_link[\"codes\"] = candidates, codes\n",
    "\n",
    "if HARD_TRIPLETS_TYPE == 'top':\n",
    "    df_hard_triplets = TopHardTriplets(df_train_link).generate_triplets(NUM_NEGATIVES)\n",
    "elif HARD_TRIPLETS_TYPE == 'sim':\n",
    "    df_train_link[\"similarities\"] = similarities\n",
    "    df_hard_triplets = SimilarityHardTriplets(df_train_link).generate_triplets(similarity_threshold=0.35)\n",
    "elif HARD_TRIPLETS_TYPE == 'kg':\n",
    "    with open(\"../src/utils/graph_G.pkl\", \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    with open(\"../src/utils/scui_to_cui_dict.pkl\", \"rb\") as handle:\n",
    "        mapping_dict = pickle.load(handle)\n",
    "    df_hard_triplets = HardTripletsKG(df_train_link, G, mapping_dict, DEPTH, bidirectional=False).generate_triplets()\n",
    "elif HARD_TRIPLETS_TYPE == 'bkg':\n",
    "    with open(\"../src/utils/graph_G.pkl\", \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    with open(\"scui_to_cui_dict.pkl\", \"rb\") as handle:\n",
    "        mapping_dict = pickle.load(handle)\n",
    "    df_hard_triplets = HardTripletsKG(df_train_link, G, mapping_dict, DEPTH, bidirectional=True).generate_triplets()\n",
    "\n",
    "df_hard_triplets = df_hard_triplets.drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auscultación pulmonar</td>\n",
       "      <td>auscultación del tracto respiratorio inferior</td>\n",
       "      <td>incentivo de una conducta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auscultación pulmonar</td>\n",
       "      <td>auscultación del tracto respiratorio inferior</td>\n",
       "      <td>valvulotomía pulmonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auscultación pulmonar</td>\n",
       "      <td>auscultación del tracto respiratorio inferior</td>\n",
       "      <td>procedimiento de medición</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auscultación pulmonar</td>\n",
       "      <td>auscultación del tracto respiratorio inferior</td>\n",
       "      <td>percusión mediata (procedimiento)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>auscultación pulmonar</td>\n",
       "      <td>auscultación del tracto respiratorio inferior</td>\n",
       "      <td>condensado de aliento exhalado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  anchor                                       positive  \\\n",
       "0  auscultación pulmonar  auscultación del tracto respiratorio inferior   \n",
       "1  auscultación pulmonar  auscultación del tracto respiratorio inferior   \n",
       "2  auscultación pulmonar  auscultación del tracto respiratorio inferior   \n",
       "3  auscultación pulmonar  auscultación del tracto respiratorio inferior   \n",
       "4  auscultación pulmonar  auscultación del tracto respiratorio inferior   \n",
       "\n",
       "                            negative  \n",
       "0          incentivo de una conducta  \n",
       "1              valvulotomía pulmonar  \n",
       "2          procedimiento de medición  \n",
       "3  percusión mediata (procedimiento)  \n",
       "4     condensado de aliento exhalado  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hard_triplets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cambridgeltl/SapBERT-UMLS-2020AB-all-lang-from-XLMR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPUs!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e0569ca3964bc780bbc47913d48633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0904c51ae16e48ec89c9147da81a8efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/19092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fernandogd/comun/fernando/KnowledgeGraph/notebooks/Cross-encoder_Training.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bisolda/home/fernandogd/comun/fernando/KnowledgeGraph/notebooks/Cross-encoder_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cross_encoder \u001b[39m=\u001b[39m CrossEncoderReranker(MODEL_PATH, model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m, max_seq_length\u001b[39m=\u001b[39mMAX_LENGTH)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bisolda/home/fernandogd/comun/fernando/KnowledgeGraph/notebooks/Cross-encoder_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m cross_encoder\u001b[39m.\u001b[39;49mtrain(df_hard_triplets, output_path, BATCH_SIZE, EPOCHS, evaluator_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, optimizer_parameters\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m:LR}, weight_decay\u001b[39m=\u001b[39;49mWEIGHT_DECAY, evaluation_steps\u001b[39m=\u001b[39;49mEVAL_STEPS, save_best_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, test_size\u001b[39m=\u001b[39;49mTEST_SIZE)\n",
      "File \u001b[0;32m/home/comun/fernando/KnowledgeGraph/notebooks/../src/crossEncoder.py:60\u001b[0m, in \u001b[0;36mCrossEncoderReranker.train\u001b[0;34m(self, df_hard_triplets, output_path, batch_size, epochs, evaluator_type, optimizer_parameters, weight_decay, evaluation_steps, save_best_model, test_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39melif\u001b[39;00m evaluator_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCERankingEvaluator\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     57\u001b[0m     evaluator \u001b[39m=\u001b[39m CERerankingEvaluator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_triplets_rankingeval(dev_samples), name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, evaluator\u001b[39m=\u001b[39;49mevaluator, epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     61\u001b[0m              optimizer_params\u001b[39m=\u001b[39;49moptimizer_parameters, weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m     62\u001b[0m              evaluation_steps\u001b[39m=\u001b[39;49mevaluation_steps, warmup_steps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(train_samples) \u001b[39m/\u001b[39;49m batch_size \u001b[39m*\u001b[39;49m epochs \u001b[39m*\u001b[39;49m \u001b[39m0.1\u001b[39;49m),\n\u001b[1;32m     63\u001b[0m              output_path\u001b[39m=\u001b[39;49moutput_path, save_best_model\u001b[39m=\u001b[39;49msave_best_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:238\u001b[0m, in \u001b[0;36mCrossEncoder.fit\u001b[0;34m(self, train_dataloader, evaluator, epochs, loss_fct, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m--> 238\u001b[0m \u001b[39mfor\u001b[39;00m features, labels \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m    239\u001b[0m     train_dataloader, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration\u001b[39m\u001b[39m\"\u001b[39m, smoothing\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar\n\u001b[1;32m    240\u001b[0m ):\n\u001b[1;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m use_amp:\n\u001b[1;32m    242\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_device\u001b[39m.\u001b[39mtype):\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/kg_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:124\u001b[0m, in \u001b[0;36mCrossEncoder.smart_batching_collate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    119\u001b[0m     labels\u001b[39m.\u001b[39mappend(example\u001b[39m.\u001b[39mlabel)\n\u001b[1;32m    121\u001b[0m tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    122\u001b[0m     \u001b[39m*\u001b[39mtexts, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlongest_first\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(labels, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_labels \u001b[39m==\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39melse\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mlong)\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_device\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m tokenized:\n\u001b[1;32m    129\u001b[0m     tokenized[name] \u001b[39m=\u001b[39m tokenized[name]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_encoder = CrossEncoderReranker(MODEL_PATH, model_type=\"mask\", max_seq_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.prepare_triplets(df_hard_triplets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
